---
title: "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers"
collection: publications
permalink: /publication/papa
date: 2022-01-09
venue: 'Findings of the Association for Computational Linguistics: EMNLP 2022'
paperurl: 'https://arxiv.org/abs/2211.03495'
link: 'https://arxiv.org/abs/2211.03495'
citation: 'Michael Hassid, Hao Peng, Daniel Rotem, <b>Jungo Kasai</b>, Ivan Montero, Noah A. Smith, and Roy Schwartz. 2022. &quot;How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers.&quot; <i>Findings of the Association for Computational Linguistics: EMNLP 2022</i>.'
---

